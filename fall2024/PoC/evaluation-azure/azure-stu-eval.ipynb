{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ade53f-41d9-4929-b8a6-c7bda2487903",
   "metadata": {},
   "source": [
    "# Azure Evaluation On Stuttering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91f32c-9f44-445e-af1d-d80438c80a73",
   "metadata": {},
   "source": [
    "1. Turn original transcripts into dataframes\n",
    "2. Read generated transcript dataframes\n",
    "3. Get Error & Stuttering count\n",
    "4. Get Error & Stuttering Type ==> show which Stuttering type is more prone to error\n",
    "5. Calculate Correlation Score\n",
    "6. Generate Heatmap\n",
    "7. Summarize Trend\n",
    "\n",
    "*** \n",
    "\n",
    "### Error Metrics\n",
    "1. Net / Total Word Error Rate\n",
    "2. Word Error Rate Specific After Cleaning other Stuttering Type Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3447c6-7204-47d4-90e4-c5cfdea58819",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea686dc-bce5-4d46-9064-b11e197e5a78",
   "metadata": {},
   "source": [
    "## Using Custom Kernel on SCC\n",
    "\n",
    "SCC sometimes has the problem with installed library not importable [`module not found` error], this is an alternative.\n",
    "\n",
    "Assuming you have a conda environment created, you would do the following:\n",
    "1. `conda install -c anaconda ipykernel` \n",
    "2. `python -m ipykernel install --user --name=<env name>`\n",
    "3. If the new kernel cannot be found, relaunch a new SCC instance\n",
    "\n",
    "**Remember to switch to the conda env kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f1428-92a8-4db0-829c-f30a8146f0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_transcript_path = \"/projectnb/ds549/projects/AImpower/datasets/updated_annotation_deid_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a20da-9100-47fe-b6b4-acd844df0194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy scipy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c243649-dc9e-45da-84b9-1698d77b8232",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38fa21-278b-424f-8346-a22b6090b43a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word Error Rate\n",
    "\n",
    "Objectives:\n",
    "* split sequence into characters\n",
    "* count:\n",
    "    * deletion: missing words\n",
    "    * substitutions: wrongly recognized words\n",
    "    * insertions: extra words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81df43-e7a5-4fc8-bd91-aea09d092265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wer(candidate, reference):\n",
    "    \"\"\"\n",
    "    Parameter(s)\n",
    "    ------------\n",
    "    candidate ==> generated transcript\n",
    "    reference ==> dataset transcript\n",
    "    \"\"\"\n",
    "    \n",
    "    candidate_tokens = list(candidate)\n",
    "    reference_tokens = list(reference)\n",
    "    \n",
    "    cand_len = len(candidate_tokens)\n",
    "    ref_len = len(reference_tokens)\n",
    "    \n",
    "    dist_mat = np.zeros((ref_len, cand_len), dtype=int)\n",
    "    \n",
    "    for i in range(ref_len):\n",
    "        dist_mat[i][0] = i\n",
    "    for j in range(cand_len):\n",
    "        dist_mat[0][j] = j\n",
    "        \n",
    "    for i in range(1, ref_len):\n",
    "        for j in range(1, cand_len):\n",
    "            if (candidate_tokens[j - 1] == reference_tokens[i - 1]):\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "                \n",
    "            dist_mat[i][j] = min(\n",
    "                dist_mat[i-1][j] + 1,\n",
    "                dist_mat[i][j-1] + 1,\n",
    "                dist_mat[i-1][j-1] + cost\n",
    "            )\n",
    "            # print(dist_mat)\n",
    "            \n",
    "    wer = dist_mat[-1][-1] / len(reference_tokens)\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febcf1a3-f6cc-4a91-adba-bcc77778de63",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a446d7-0d6b-4dee-bab6-04f07a352782",
   "metadata": {},
   "source": [
    "## Imports and Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b0091-575e-456b-98b3-512203307bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42a3b2-7d82-49c2-8ef9-78072b41ea77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_data = pd.DataFrame(columns=[\"Filename\", \"Start_time\", \"End_time\", \"Transcript\"]) \n",
    "net_aigenerated_data_azure = pd.read_csv(\"/projectnb/ds549/projects/AImpower/datasets/generated-transcripts/Azure.csv\", delimiter=\",\")\n",
    "\n",
    "del net_aigenerated_data_azure[net_aigenerated_data_azure.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9f360-6023-4820-8e62-0fec34f36acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for folder in os.listdir(ds_transcript_path):\n",
    "    if folder == \"command_stats.xlsx\" or folder == \"command_stats.csv\":\n",
    "        continue\n",
    "    for audio_sample in os.listdir(os.path.join(ds_transcript_path, f\"{folder}\")):\n",
    "        if (\"_A.txt\" in audio_sample):\n",
    "            net_data = pd.concat([net_data, pd.read_csv(os.path.join(ds_transcript_path, f\"{folder}/{audio_sample}\"), sep=\"\\t\", names=[\"Start_time\", \"End_time\", \"Transcript\"]).assign(Filename=f\"D{folder}_A\")])\n",
    "        if (\"_B.txt\" in audio_sample):\n",
    "            net_data = pd.concat([net_data, pd.read_csv(os.path.join(ds_transcript_path, f\"{folder}/{audio_sample}\"), sep=\"\\t\", names=[\"Start_time\", \"End_time\", \"Transcript\"]).assign(Filename=f\"D{folder}_B\")])\n",
    "        if (\"P\" in audio_sample):\n",
    "            net_data = pd.concat([net_data, pd.read_csv(os.path.join(ds_transcript_path, f\"{folder}/{audio_sample}\"), sep=\"\\t\", names=[\"Start_time\", \"End_time\", \"Transcript\"]).assign(Filename=f\"P{folder}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce2b50-9c43-40a6-ae23-69ed8676ff3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_pattern = r\"\\<.*?\\>\"\n",
    "repetition_pattern = r\"\\[.*?\\]\"\n",
    "annotation_pattern = r\"/\\w\"\n",
    "\n",
    "\n",
    "net_data = net_data.assign(Cleaned_Transcript=net_data['Transcript'].apply(lambda x: re.sub(annotation_pattern, \"\", re.sub(repetition_pattern, \"\", re.sub(mask_pattern, \"\", x)))))\n",
    "net_data = net_data.assign(Stutterance_Count=net_data['Transcript'].apply(lambda x: len(re.findall(mask_pattern, x)) + len(re.findall(repetition_pattern, x)) + len(re.findall(annotation_pattern, x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f8b87-c5bc-453f-9399-efef62a00a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8c032-607b-4eb7-ac99-6d82c28d846a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_aigenerated_data_azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70509616-62dc-494d-9914-f96010242e1c",
   "metadata": {},
   "source": [
    "**Now we have raw data of all audio transcriptions from datasets [updated_annotation_deid_full] in `net_data` and AI predicted transcriptions in `net_aigenerated_data_azure`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced1999f-2d44-4709-8169-719369c1067c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e2625-861c-4bd9-a7b2-82db58f4e931",
   "metadata": {},
   "source": [
    "## WER Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a194fb38-ea3c-4028-92be-c106a00e84a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "na_count_large = 0\n",
    "na_count_cleaned = 0\n",
    "for index, row in tqdm(net_aigenerated_data_azure.iterrows(), total=len(net_aigenerated_data_azure)):\n",
    "    \n",
    "    mask_large = (\n",
    "        (net_aigenerated_data_azure[\"Filename\"] == row[\"Filename\"]) &\n",
    "        (net_aigenerated_data_azure[\"Start_time\"] == row[\"Start_time\"])\n",
    "    )\n",
    "\n",
    "    mask_net = (\n",
    "        (net_data[\"Filename\"] == row[\"Filename\"]) &\n",
    "        (net_data[\"Start_time\"] == row[\"Start_time\"])\n",
    "    )\n",
    "\n",
    "    \n",
    "    large_row = net_aigenerated_data_azure.loc[mask_large]\n",
    "    net_row = net_data.loc[mask_net]\n",
    "\n",
    "    # print(large_row)\n",
    "    # print('\\n\\n\\n\\n')\n",
    "    # print(net_row)\n",
    "    \n",
    "    if large_row.empty or net_row.empty:\n",
    "        print(\"Skipping: One of the rows is empty.\")\n",
    "        continue\n",
    "        \n",
    "    azure = large_row[\"Azure\"].values[0]\n",
    "    cleaned_transcript = net_row[\"Cleaned_Transcript\"].values[0]\n",
    "    \n",
    "    if pd.isna(azure) or not isinstance(azure, str):\n",
    "        print(\"Skipping due to missing or non-string Azure.\")\n",
    "        na_count_large = na_count_large + 1\n",
    "        continue\n",
    "    if pd.isna(cleaned_transcript) or not isinstance(cleaned_transcript, str):\n",
    "        print(\"Skipping due to missing or non-string Cleaned_Transcript.\")\n",
    "        na_count_cleaned = na_count_cleaned + 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        \n",
    "        wer_value = wer(azure, cleaned_transcript)\n",
    "        \n",
    "        net_aigenerated_data_azure.loc[mask_large, \"WER\"] = wer_value\n",
    "        \n",
    "        stutterance_count = net_row[\"Stutterance_Count\"].values[0]\n",
    "        net_aigenerated_data_azure.loc[mask_large, \"Stutterance_Count\"] = stutterance_count\n",
    "\n",
    "        # Verify assignment\n",
    "        # print(f'Assigned Stutterance_Count: {stutterance_count}')\n",
    "        # print(net_aigenerated_data_azure.loc[mask_large, \"Stutterance_Count\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "        print('Occurred with the following data:')\n",
    "        print(large_row)\n",
    "        print(net_row)\n",
    "        \n",
    "net_aigenerated_data_azure = net_aigenerated_data_azure.assign(NA_Count=na_count_large)\n",
    "net_aigenerated_data_azure = net_aigenerated_data_azure.assign(NA_Cleaned_Count=na_count_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3731c-0be0-4659-955b-5d47f7e33137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_aigenerated_data_azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a6dfc-fa15-45c4-8c08-1682e3fb482e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_aigenerated_data_azure.to_csv('net_aigenerated_data_azure_performance_stu.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79206fa2-50ce-4c59-aece-f17439fdcf92",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd854d6-464c-49a3-a82b-c6603567fb0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization of Relationship between Stuttering Count and Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f87049-4fd6-447b-aa37-1d3bb808a1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load data from csv if starting here\n",
    "\n",
    "net_aigenerated_data_azure = pd.read_csv('/projectnb/ds549/projects/AImpower/evaluation-azure/net_aigenerated_data_azure_performance_stu.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a7d2b-f110-4b04-8500-1a956ebf78be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d5c7c-bc01-4dfc-959f-589115e8fd75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Null value plots\n",
    "\n",
    "nonnull_count_large = net_aigenerated_data_azure[\"NA_Count\"].count() - net_aigenerated_data_azure.iloc[0][\"NA_Count\"]\n",
    "null_count_large = net_aigenerated_data_azure.iloc[0][\"NA_Count\"]\n",
    "\n",
    "nonnull_count_cleaned = net_aigenerated_data_azure[\"NA_Cleaned_Count\"].count() - net_aigenerated_data_azure.iloc[0][\"NA_Cleaned_Count\"]\n",
    "null_count_cleaned = net_aigenerated_data_azure.iloc[0][\"NA_Cleaned_Count\"]\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"NA Values\": [null_count_large, null_count_cleaned],\n",
    "    \"Non NA Values\": [nonnull_count_large, nonnull_count_cleaned],\n",
    "}\n",
    "\n",
    "species = (\n",
    "    \"Azure\",\n",
    "    \"Cleaned Ground Truth\"\n",
    ")\n",
    "\n",
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(2)\n",
    "\n",
    "for na, count in data.items():\n",
    "    p = ax.bar(species, count, width, label=na, bottom=bottom)\n",
    "    bottom += count\n",
    "\n",
    "\n",
    "ax.set_title(\"NA value counting\", fontsize=16)\n",
    "ax.set_xlabel(\"Source\", fontsize=14)\n",
    "ax.set_ylabel(\"Count\", fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b57777-464a-43d5-b3c1-510ae5b7f459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"WER\"], \n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.title(\"WER vs Stutterance Count\", fontsize=16)\n",
    "plt.xlabel(\"Stutterance Count\", fontsize=14)\n",
    "plt.ylabel(\"WER\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787eb1b-8b2f-45bd-aca7-2e6490f4a56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "net_aigenerated_data_azure['WER_Binned'] = np.round(net_aigenerated_data_azure['WER'], 2)\n",
    "\n",
    "grouped_data = net_aigenerated_data_azure.groupby(\n",
    "    ['Stutterance_Count', 'WER_Binned']\n",
    ").size().reset_index(name='Count')\n",
    "heatmap_data = grouped_data.pivot(index='WER_Binned', columns='Stutterance_Count', values='Count').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    heatmap_data, cmap='cool', annot=False, fmt='g', cbar=True\n",
    ")\n",
    "\n",
    "plt.title(\"Stutterance Count vs WER (Color = Number of Cases)\", fontsize=16)\n",
    "plt.xlabel(\"Stutterance Count\", fontsize=14)\n",
    "plt.ylabel(\"WER (Binned)\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655e4f3-8870-45a1-bdcc-cbe54d532f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "rho, p = spearmanr(net_aigenerated_data_azure.dropna()['Stutterance_Count'], net_aigenerated_data_azure.dropna()['WER'])\n",
    "print(f\"p-value = {p}\")\n",
    "print(f\"rho = {rho}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6bf6e-2993-485d-9141-a78634637a50",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e4ac0-04a6-4896-b5b4-cbcfc330e0d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ROUGE-N/L Scores (Semantic Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ed190-1369-45ce-bd2e-863505f99a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_chinese import Rouge\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1133e-38f4-4798-8bfe-7afb95778756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee381a-b7b8-4dd5-8e76-d7a8f3483e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_ = True\n",
    "\n",
    "for index, row in tqdm(net_aigenerated_data_azure.iterrows(), total=len(net_aigenerated_data_azure)):\n",
    "    \n",
    "    mask_large = (\n",
    "        (net_aigenerated_data_azure[\"Filename\"] == row[\"Filename\"]) &\n",
    "        (net_aigenerated_data_azure[\"Start_time\"] == row[\"Start_time\"])\n",
    "    )\n",
    "\n",
    "    mask_net = (\n",
    "        (net_data[\"Filename\"] == row[\"Filename\"]) &\n",
    "        (net_data[\"Start_time\"] == row[\"Start_time\"])\n",
    "    )\n",
    "\n",
    "    \n",
    "    large_row = net_aigenerated_data_azure.loc[mask_large]\n",
    "    net_row = net_data.loc[mask_net]\n",
    "\n",
    "    # print(large_row)\n",
    "    # print('\\n\\n\\n\\n')\n",
    "    # print(net_row)\n",
    "    \n",
    "    if large_row.empty or net_row.empty:\n",
    "        print(\"Skipping: One of the rows is empty.\")\n",
    "        continue\n",
    "        \n",
    "    azure = large_row[\"Azure\"].values[0]\n",
    "    cleaned_transcript = net_row[\"Cleaned_Transcript\"].values[0]\n",
    "    \n",
    "    if pd.isna(azure) or not isinstance(azure, str):\n",
    "        print(\"Skipping due to missing or non-string Azure.\")\n",
    "        continue\n",
    "    if pd.isna(cleaned_transcript) or not isinstance(cleaned_transcript, str):\n",
    "        print(\"Skipping due to missing or non-string Cleaned_Transcript.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        \n",
    "        scores = rouge.get_scores(' '.join(jieba.cut(azure)), ' '.join(jieba.cut(cleaned_transcript)))\n",
    "        \n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rouge1-precision\"] = scores[0][\"rouge-1\"][\"p\"]\n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rouge1-recall\"] = scores[0][\"rouge-1\"][\"r\"]\n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rouge1-f1\"] = scores[0][\"rouge-1\"][\"f\"]\n",
    "        \n",
    "        \n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rouge2-precision\"] = scores[0][\"rouge-2\"][\"p\"]\n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rouge2-recall\"] = scores[0][\"rouge-2\"][\"r\"]\n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rouge2-f1\"] = scores[0][\"rouge-2\"][\"f\"]\n",
    "        \n",
    "        \n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rougel-precision\"] = scores[0][\"rouge-l\"][\"p\"]\n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rougel-recall\"] = scores[0][\"rouge-l\"][\"r\"]\n",
    "        net_aigenerated_data_azure.loc[mask_large, \"rougel-f1\"] = scores[0][\"rouge-l\"][\"f\"]\n",
    "        \n",
    "        stutterance_count = net_row[\"Stutterance_Count\"].values[0]\n",
    "        net_aigenerated_data_azure.loc[mask_large, \"Stutterance_Count\"] = stutterance_count\n",
    "\n",
    "        if (print_):\n",
    "            print(net_aigenerated_data_azure)\n",
    "            print_ = False\n",
    "        \n",
    "        # Verify assignment\n",
    "        # print(f'Assigned Stutterance_Count: {stutterance_count}')\n",
    "        # print(net_aigenerated_data_azure.loc[mask_large, \"Stutterance_Count\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "        print('Occurred with the following data:')\n",
    "        print(large_row)\n",
    "        print(net_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bc54a-90fb-409a-a30d-d90ddf2550cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_aigenerated_data_azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cba2a5-5178-4579-b0a0-5ec8b89181fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_aigenerated_data_azure.to_csv('net_aigenerated_data_azure_performance_stu.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dd59d-7ebd-49bd-8d18-acae4780be3c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e91e6-4f9d-423a-9d57-d19ba960bac1",
   "metadata": {},
   "source": [
    "## Visualization of Relationship between Stuttering Count and Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf03ee0-99ec-4f0a-8a29-bc6d6e53bcb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load data from csv if starting here\n",
    "\n",
    "net_aigenerated_data_azure = pd.read_csv('/projectnb/ds549/projects/AImpower/evaluation-azure/net_aigenerated_data_azure_performance_stu.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e1afc-5996-44a9-a70a-feb0cdd29237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rouge1-precision\"], \n",
    "    facecolors=\"none\", edgecolors='r',\n",
    "    marker=\"8\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rouge1-recall\"], \n",
    "    facecolors=\"none\", edgecolors='g',\n",
    "    marker=\"^\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rouge1-f1\"], \n",
    "    facecolors=\"none\", edgecolors='b',\n",
    "    marker=\".\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.title(\"Rouge-1 vs Stutterance Count\", fontsize=16)\n",
    "plt.xlabel(\"Stutterance Count\", fontsize=14)\n",
    "plt.ylabel(\"Rouge Score\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7c856-ac25-4ad8-af68-94b7603e413f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rouge2-precision\"], \n",
    "    facecolors=\"none\", edgecolors='r',\n",
    "    marker=\"8\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rouge2-recall\"], \n",
    "    facecolors=\"none\", edgecolors='g',\n",
    "    marker=\"^\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rouge2-f1\"], \n",
    "    facecolors=\"none\", edgecolors='b',\n",
    "    marker=\".\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.title(\"Rouge-2 vs Stutterance Count\", fontsize=16)\n",
    "plt.xlabel(\"Stutterance Count\", fontsize=14)\n",
    "plt.ylabel(\"Rouge Score\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aacd2e-8b9a-49e0-b93a-61cd141809ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rougel-precision\"], \n",
    "    facecolors=\"none\", edgecolors='r',\n",
    "    marker=\"8\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rougel-recall\"], \n",
    "    facecolors=\"none\", edgecolors='g',\n",
    "    marker=\"^\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    net_aigenerated_data_azure[\"Stutterance_Count\"], \n",
    "    net_aigenerated_data_azure[\"rougel-f1\"], \n",
    "    facecolors=\"none\", edgecolors='b',\n",
    "    marker=\".\",\n",
    "    alpha=0.7  # Handle overlapping points\n",
    ")\n",
    "\n",
    "plt.title(\"Rouge-L vs Stutterance Count\", fontsize=16)\n",
    "plt.xlabel(\"Stutterance Count\", fontsize=14)\n",
    "plt.ylabel(\"Rouge Score\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec04e9-605d-4d66-ab46-87f8441a790c",
   "metadata": {},
   "source": [
    "## Correlations between Stuttering and Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1aeda0-f917-4f4c-b5aa-bf5fe18f17f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "rho, p = spearmanr(net_aigenerated_data_azure.dropna()['Stutterance_Count'], net_aigenerated_data_azure.dropna()['rouge1-precision'])\n",
    "print(f\"p-value [stuttering count & rouge-1 precision] = {p}\")\n",
    "print(f\"rho [stuttering count & rouge-1 precision] = {rho}\")\n",
    "\n",
    "rho, p = spearmanr(net_aigenerated_data_azure.dropna()['Stutterance_Count'], net_aigenerated_data_azure.dropna()['rouge1-recall'])\n",
    "print(f\"p-value [stuttering count & rouge-1 recall] = {p}\")\n",
    "print(f\"rho [stuttering count & rouge-1 recall] = {rho}\")\n",
    "\n",
    "rho, p = spearmanr(net_aigenerated_data_azure.dropna()['Stutterance_Count'], net_aigenerated_data_azure.dropna()['rouge1-f1'])\n",
    "print(f\"p-value [stuttering count & rouge-1 f1] = {p}\")\n",
    "print(f\"rho [stuttering count & rouge-1 f1] = {rho}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learner-env)",
   "language": "python",
   "name": "learner-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
