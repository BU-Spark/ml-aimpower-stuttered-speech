{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install jiwer\n",
    "# !pip3 install nltk\n",
    "# !pip3 install wn\n",
    "# !pip3 install opencc\n",
    "# !pip3 install sent2vec\n",
    "# !pip3 install transformers torch scikit-learn\n",
    "# !pip3 install pynlpir\n",
    "# !pip3 install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr2/collab/dlee5/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jiwer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "nltk.download('punkt')\n",
    "import jieba\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/howto/wordnet.html\n",
    "# https://stackoverflow.com/questions/63514884/does-wordnet-python-nltk-interface-includes-any-measure-of-semantic-relatedness\n",
    "def calculate_wordnet_first(reference, hypothesis):\n",
    "    ref_lemmas = [wn.lemmas(word, lang='cmn')[0] for word in jieba.lcut(reference) if wn.lemmas(word, lang='cmn')] # only top 1\n",
    "    hyp_lemmas = [wn.lemmas(word, lang='cmn')[0] for word in jieba.lcut(hypothesis) if wn.lemmas(word, lang='cmn')]\n",
    "    similarities = []\n",
    "    for ref_lemma in ref_lemmas:\n",
    "        for hyp_lemma in hyp_lemmas:\n",
    "            similarity = wn.wup_similarity(ref_lemma.synset(), hyp_lemma.synset())\n",
    "            if similarity:\n",
    "                similarities.append(similarity)\n",
    "    if similarities:\n",
    "        return sum(similarities) / len(similarities)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def calculate_wordnet_all(reference, hypothesis):\n",
    "    ref_lemmas = [lemmas for word in jieba.lcut(reference) for lemmas in wn.lemmas(word, lang='cmn')] # consider all candidates\n",
    "    hyp_lemmas = [lemmas for word in jieba.lcut(hypothesis) for lemmas in wn.lemmas(word, lang='cmn')]\n",
    "    similarities = []\n",
    "    for ref_lemma in ref_lemmas:\n",
    "        for hyp_lemma in hyp_lemmas:\n",
    "            similarity = wn.wup_similarity(ref_lemma.synset(), hyp_lemma.synset())\n",
    "            if similarity:\n",
    "                similarities.append(similarity)\n",
    "    if similarities:\n",
    "        return sum(similarities) / len(similarities)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# https://github.com/stanfordnlp/GloVe # only has English\n",
    "# https://www.kaggle.com/datasets/chongjiujjin/chinese-word-embedding-glove-dim-128?resource=download # Chinese vector embeddings\n",
    "glove_embeddings = {}\n",
    "with open('vectors.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "def calculate_glove(reference, hypothesis):\n",
    "    ref_vectors = [glove_embeddings[word] for word in jieba.lcut(reference) if word in glove_embeddings]\n",
    "    hyp_vectors = [glove_embeddings[word] for word in jieba.lcut(hypothesis) if word in glove_embeddings]\n",
    "    if not ref_vectors or not hyp_vectors:\n",
    "        return 0.0\n",
    "    ref_mean_vector = np.mean(ref_vectors, axis=0).reshape(1, -1)\n",
    "    hyp_mean_vector = np.mean(hyp_vectors, axis=0).reshape(1, -1)\n",
    "    return cosine_similarity(ref_mean_vector, hyp_mean_vector)[0][0]\n",
    "\n",
    "\n",
    "# https://huggingface.co/google-bert/bert-base-chinese # 'bert-base-chinese'\n",
    "## https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md # advanced CWS included version\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('hfl/rbt3')\n",
    "bert_model = BertModel.from_pretrained('hfl/rbt3')\n",
    "def calculate_bert(reference, hypothesis):\n",
    "    ref_inputs = bert_tokenizer(reference, return_tensors='pt')\n",
    "    hyp_inputs = bert_tokenizer(hypothesis, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        ref_outputs = bert_model(**ref_inputs)\n",
    "        hyp_outputs = bert_model(**hyp_inputs)\n",
    "    ref_embedding = ref_outputs.last_hidden_state.mean(dim=1)\n",
    "    hyp_embedding = hyp_outputs.last_hidden_state.mean(dim=1)\n",
    "    return cosine_similarity(ref_embedding, hyp_embedding)[0][0].item()\n",
    "\n",
    "\n",
    "# https://pypi.org/project/sent2vec/\n",
    "def calculate_sent2vec(reference, hypothesis):\n",
    "    vectorizer = Vectorizer()\n",
    "    vectorizer.run([reference, hypothesis,])\n",
    "    vectors = vectorizer.vectors\n",
    "    return cosine(vectors[0], vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_cleaned = pd.read_csv('../ground_truth_cleaned/D00XX_A_ground_truth_cleaned.csv', index_col=0)\n",
    "df_B_cleaned = pd.read_csv('../ground_truth_cleaned/D00XX_B_ground_truth_cleaned.csv', index_col=0)\n",
    "df_P_cleaned = pd.read_csv('../ground_truth_cleaned/P00XX_ground_truth_cleaned.csv', index_col=0)\n",
    "df_A_cleaned['file'] = df_A_cleaned['file'].str.extract(r'/(D\\d{4}_A)', expand=False)\n",
    "df_B_cleaned['file'] = df_B_cleaned['file'].str.extract(r'/(D\\d{4}_B)', expand=False)\n",
    "df_P_cleaned['file'] = df_P_cleaned['file'].str.extract(r'/(P\\d{4})', expand=False)\n",
    "\n",
    "def generate_evaluation_df(modelname):\n",
    "\n",
    "    df_model = pd.read_csv(f'../predicted_transcription/Whisper_tiny.csv', index_col=0)\n",
    "    df_model = df_model.rename(columns={'Filename': 'file', 'Start_time': 'start_time', 'End_time': 'end_time'})\n",
    "    df_merged = pd.merge(df_A_cleaned, df_model, on=['file', 'start_time', 'end_time'], how='outer')\n",
    "    df_merged = pd.merge(df_B_cleaned, df_merged, on=['file', 'start_time', 'end_time'], how='right', suffixes=('_x',''))\n",
    "    df_merged['participant'] = df_merged['participant'].combine_first(df_merged['participant_x'])\n",
    "    df_merged['ground_truth'] = df_merged['ground_truth'].combine_first(df_merged['ground_truth_x'])\n",
    "    df_merged['ground_truth_cleaned'] = df_merged['ground_truth_cleaned'].combine_first(df_merged['ground_truth_cleaned_x'])\n",
    "    df_merged = df_merged.drop(columns=['participant_x', 'ground_truth_x', 'ground_truth_cleaned_x'])\n",
    "    df_merged = pd.merge(df_merged, df_P_cleaned, on=['file', 'start_time', 'end_time'], how='outer', suffixes=('','_x'))\n",
    "    df_merged['participant'] = df_merged['participant'].combine_first(df_merged['participant_x'])\n",
    "    df_merged['ground_truth'] = df_merged['ground_truth'].combine_first(df_merged['ground_truth_x'])\n",
    "    df_merged['ground_truth_cleaned'] = df_merged['ground_truth_cleaned'].combine_first(df_merged['ground_truth_cleaned_x'])\n",
    "    df_merged = df_merged.drop(columns=['participant_x', 'ground_truth_x', 'ground_truth_cleaned_x'])\n",
    "\n",
    "    WER = []\n",
    "    CER = []\n",
    "    BLEU = []\n",
    "    WordNet_first = []\n",
    "    WordNet_all = []\n",
    "    GloVe = []\n",
    "    BERT = []\n",
    "    Sent2Vec = []\n",
    "    for i in range(df_merged.shape[0]):\n",
    "        reference = df_merged['ground_truth_cleaned'].iloc[i]\n",
    "        hypothesis = df_merged[modelname].iloc[i]\n",
    "        if not isinstance(reference, str) or not isinstance(hypothesis, str):\n",
    "            WER.append(np.nan)\n",
    "            CER.append(np.nan)\n",
    "            BLEU.append(np.nan)\n",
    "            WordNet_first.append(np.nan)\n",
    "            WordNet_all.append(np.nan)\n",
    "            GloVe.append(np.nan)\n",
    "            BERT.append(np.nan)\n",
    "            Sent2Vec.append(np.nan)\n",
    "        else:\n",
    "            WER.append(jiwer.wer(' '.join(jieba.lcut(reference)), ' '.join(jieba.lcut(hypothesis))))\n",
    "            CER.append(jiwer.cer(reference, hypothesis))\n",
    "            BLEU.append(sentence_bleu([jieba.lcut(reference)], jieba.lcut(hypothesis)))\n",
    "            WordNet_first.append(calculate_wordnet_first(reference, hypothesis))\n",
    "            WordNet_all.append(calculate_wordnet_all(reference, hypothesis))\n",
    "            GloVe.append(calculate_glove(reference, hypothesis))\n",
    "            BERT.append(calculate_bert(reference, hypothesis))\n",
    "            Sent2Vec.append(calculate_sent2vec(reference, hypothesis))\n",
    "        \n",
    "    df_merged['WER'] = WER\n",
    "    df_merged['CER'] = CER\n",
    "    df_merged['BLEU'] = BLEU\n",
    "    df_merged['WordNet_first'] = WordNet_first\n",
    "    df_merged['WordNet_all'] = WordNet_all\n",
    "    df_merged['GloVe'] = GloVe\n",
    "    df_merged['BERT'] = BERT\n",
    "    df_merged['Sent2Vec'] = Sent2Vec\n",
    "\n",
    "    df_merged.to_csv(f'{modelname}_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames = ['Azure', 'GoogleCloud', 'Wav2vec', 'WeNet', 'Whisper_tiny', 'Whisper_large']\n",
    "for modelname in modelnames:\n",
    "    generate_evaluation_df(modelname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spring-2024-pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
